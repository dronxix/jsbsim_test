"""
Policy Selector System –¥–ª—è Air-to-Air Combat
–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã–±–æ—Ä–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.models import ModelCatalog
from ray.rllib.policy.sample_batch import SampleBatch
from ray.rllib.algorithms.ppo.ppo_torch_policy import PPOTorchPolicy
from enum import IntEnum
import json
from dataclasses import dataclass
from collections import deque

# –¢–∏–ø—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫
class PolicyType(IntEnum):
    OBSERVER = 0          # –ù–∞–±–ª—é–¥–∞—Ç–µ–ª—å - –ø–∞—Å—Å–∏–≤–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞
    AGGRESSIVE_SHOOTER = 1 # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å—Ç—Ä–µ–ª–æ–∫ - –∞–∫—Ç–∏–≤–Ω–∞—è –∞—Ç–∞–∫–∞
    NORMAL_SHOOTER = 2    # –û–±—ã—á–Ω—ã–π —Å—Ç—Ä–µ–ª–æ–∫ - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
    DEFENSIVE = 3         # –û–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞
    INTERCEPTOR = 4       # –ü–µ—Ä–µ—Ö–≤–∞—Ç—á–∏–∫ - –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–π
    SUPPORT = 5          # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–æ—é–∑–Ω–∏–∫–æ–≤

@dataclass
class SituationContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫–∏"""
    # –¢–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Ç—É–∞—Ü–∏—è
    enemy_count: int
    ally_count: int
    enemy_distance_avg: float
    enemy_distance_min: float
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∞–º–æ–ª–µ—Ç–∞
    own_hp: float
    own_fuel: float
    own_altitude_relative: float
    own_speed: float
    
    # –í–æ–æ—Ä—É–∂–µ–Ω–∏–µ
    missiles_remaining: int
    cannon_ammo: int
    
    # –£–≥—Ä–æ–∑—ã
    incoming_missiles: int
    radar_locked: bool
    enemy_advantage: bool
    
    # –ò—Å—Ç–æ—Ä–∏—è
    recent_hits_taken: int
    recent_hits_given: int
    time_since_last_engagement: float

class SituationAnalyzer(nn.Module):
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏"""
    
    def __init__(self, obs_dim: int, context_dim: int = 32):
        super().__init__()
        self.obs_dim = obs_dim
        self.context_dim = context_dim
        
        # –≠–Ω–∫–æ–¥–µ—Ä –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–∏—Ç—É–∞—Ü–∏–∏
        self.context_encoder = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, context_dim),
            nn.Tanh()
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–∞ —Å–∏—Ç—É–∞—Ü–∏–∏
        self.situation_classifier = nn.Sequential(
            nn.Linear(context_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 6)  # 6 —Ç–∏–ø–æ–≤ —Å–∏—Ç—É–∞—Ü–∏–π
        )
        
    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–∏—Ç—É–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        """
        context = self.context_encoder(obs)
        situation_logits = self.situation_classifier(context)
        return context, situation_logits
    
    def extract_context(self, obs_dict: Dict[str, torch.Tensor]) -> SituationContext:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–ª–æ–≤–∞—Ä—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        self_obs = obs_dict["self"]
        enemies = obs_dict["enemies"]
        allies = obs_dict["allies"]
        enemies_mask = obs_dict["enemies_mask"]
        allies_mask = obs_dict["allies_mask"]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ä–∞–≥–æ–≤ –∏ —Å–æ—é–∑–Ω–∏–∫–æ–≤
        enemy_count = int(enemies_mask.sum().item())
        ally_count = int(allies_mask.sum().item())
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –≤—Ä–∞–≥–æ–≤
        enemy_positions = enemies[:, :3]  # –ø–µ—Ä–≤—ã–µ 3 - –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        enemy_distances = torch.norm(enemy_positions, dim=-1)
        valid_enemy_distances = enemy_distances[enemies_mask > 0]
        
        if len(valid_enemy_distances) > 0:
            enemy_distance_avg = float(valid_enemy_distances.mean().item())
            enemy_distance_min = float(valid_enemy_distances.min().item())
        else:
            enemy_distance_avg = 1.0
            enemy_distance_min = 1.0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–≤–æ–µ–≥–æ —Å–∞–º–æ–ª–µ—Ç–∞
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: [pos(3), attitude(3), velocity(3), angular_vel(3), tas, mach, accel(3), fuel, hp]
        own_hp = float(self_obs[-1].item())  # –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç - HP
        own_fuel = float(self_obs[-2].item())  # –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π - —Ç–æ–ø–ª–∏–≤–æ
        own_altitude_relative = float(self_obs[2].item())  # z-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞
        own_speed = float(self_obs[12].item() if len(self_obs) > 12 else 0.5)  # –≤–æ–∑–¥—É—à–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
        
        # –ê–Ω–∞–ª–∏–∑ –≤–æ–æ—Ä—É–∂–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ info)
        missiles_remaining = 4  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –∏–∑ info
        cannon_ammo = 500
        
        # –ê–Ω–∞–ª–∏–∑ —É–≥—Ä–æ–∑ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        radar_locked = enemy_distance_min < 0.3  # –±–ª–∏–∑–∫–∏–π –≤—Ä–∞–≥ = –≤–µ—Ä–æ—è—Ç–Ω–∞—è —É–≥—Ä–æ–∑–∞
        enemy_advantage = enemy_count > ally_count
        incoming_missiles = 0  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –∏–∑ info
        
        return SituationContext(
            enemy_count=enemy_count,
            ally_count=ally_count,
            enemy_distance_avg=enemy_distance_avg,
            enemy_distance_min=enemy_distance_min,
            own_hp=own_hp,
            own_fuel=own_fuel,
            own_altitude_relative=own_altitude_relative,
            own_speed=own_speed,
            missiles_remaining=missiles_remaining,
            cannon_ammo=cannon_ammo,
            incoming_missiles=incoming_missiles,
            radar_locked=radar_locked,
            enemy_advantage=enemy_advantage,
            recent_hits_taken=0,  # –±—É–¥–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å—Å—è
            recent_hits_given=0,  # –±—É–¥–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å—Å—è
            time_since_last_engagement=0.0
        )

class PolicySelectorNetwork(nn.Module):
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫"""
    
    def __init__(self, context_dim: int = 32, num_policies: int = 6, hidden_dim: int = 64):
        super().__init__()
        self.context_dim = context_dim
        self.num_policies = num_policies
        
        # –≠–Ω–∫–æ–¥–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–∏—Ç—É–∞—Ü–∏–∏
        self.situation_encoder = nn.Sequential(
            nn.Linear(context_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # –≠–Ω–∫–æ–¥–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫
        self.history_encoder = nn.Sequential(
            nn.Linear(num_policies * 10, hidden_dim//2),  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –≤—ã–±–æ—Ä–æ–≤
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//2)
        )
        
        # –≠–Ω–∫–æ–¥–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–ª–∏—Ç–∏–∫
        self.performance_encoder = nn.Sequential(
            nn.Linear(num_policies, hidden_dim//4),  # –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏
            nn.ReLU(),
            nn.Linear(hidden_dim//4, hidden_dim//4)
        )
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ–ª–∏—Ç–∏–∫
        self.policy_selector = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2 + hidden_dim//4, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, num_policies)
        )
        
        # Value —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã–±–æ—Ä–∞
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2 + hidden_dim//4, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, 1)
        )
    
    def forward(self, situation_context: torch.Tensor, 
                policy_history: torch.Tensor,
                policy_performance: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∏—Å—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥—ã
        situation_encoded = self.situation_encoder(situation_context)
        history_encoded = self.history_encoder(policy_history)
        performance_encoded = self.performance_encoder(policy_performance)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined_features = torch.cat([
            situation_encoded, 
            history_encoded, 
            performance_encoded
        ], dim=-1)
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
        policy_logits = self.policy_selector(combined_features)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã–±–æ—Ä–∞
        value = self.value_head(combined_features)
        
        return policy_logits, value

class PolicySelector:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫"""
    
    def __init__(self, obs_space, num_policies: int = 6, selection_frequency: int = 10):
        self.num_policies = num_policies
        self.selection_frequency = selection_frequency
        self.obs_space = obs_space
        
        # –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Ç—É–∞—Ü–∏–∏
        obs_dim = self._calculate_obs_dim()
        self.situation_analyzer = SituationAnalyzer(obs_dim)
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä –ø–æ–ª–∏—Ç–∏–∫
        self.selector_network = PolicySelectorNetwork(num_policies=num_policies)
        
        # –ò—Å—Ç–æ—Ä–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.policy_history = deque(maxlen=100)  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –≤—ã–±–æ—Ä–æ–≤
        self.policy_performance = np.zeros(num_policies, dtype=np.float32)
        self.policy_usage_count = np.zeros(num_policies, dtype=np.int32)
        
        # –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.current_policy = PolicyType.NORMAL_SHOOTER
        self.steps_since_selection = 0
        self.last_situation_context = None
        
        # –û–±—É—á–µ–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
        self.selector_optimizer = torch.optim.Adam(
            list(self.situation_analyzer.parameters()) + 
            list(self.selector_network.parameters()), 
            lr=3e-4
        )
        
        print(f"üéØ Policy Selector initialized:")
        print(f"   Policies: {num_policies}")
        print(f"   Selection frequency: every {selection_frequency} steps")
        print(f"   Available policies: {[p.name for p in PolicyType][:num_policies]}")
    
    def _calculate_obs_dim(self) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        total_dim = 0
        if hasattr(self.obs_space, 'spaces'):
            for key, space in self.obs_space.spaces.items():
                if key != 'global_state':  # –∏—Å–∫–ª—é—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    if len(space.shape) == 1:
                        total_dim += space.shape[0]
                    elif len(space.shape) == 2:
                        total_dim += space.shape[0] * space.shape[1]
        return max(total_dim, 64)  # –º–∏–Ω–∏–º—É–º 64
    
    def select_policy(self, obs_dict: Dict[str, torch.Tensor], 
                     infos: Optional[Dict] = None) -> int:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        """
        self.steps_since_selection += 1
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        if self.steps_since_selection < self.selection_frequency:
            return int(self.current_policy)
        
        self.steps_since_selection = 0
        
        with torch.no_grad():
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Ç—É–∞—Ü–∏—é
            obs_flat = self._flatten_observations(obs_dict)
            situation_context, situation_logits = self.situation_analyzer(obs_flat)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫
            policy_history_tensor = self._prepare_policy_history()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫
            policy_performance_tensor = torch.from_numpy(self.policy_performance)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
            policy_logits, value = self.selector_network(
                situation_context,
                policy_history_tensor,
                policy_performance_tensor.unsqueeze(0)
            )
            
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ (—Å –Ω–µ–±–æ–ª—å—à–æ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å—é –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)
            if np.random.random() < 0.1:  # 10% –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                selected_policy = np.random.randint(0, self.num_policies)
            else:
                selected_policy = int(torch.argmax(policy_logits, dim=-1).item())
            
            self.current_policy = PolicyType(selected_policy)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.policy_usage_count[selected_policy] += 1
            self.policy_history.append({
                'policy': selected_policy,
                'situation_context': situation_context.cpu().numpy(),
                'value': float(value.item()),
                'step': len(self.policy_history)
            })
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            self.last_situation_context = self.situation_analyzer.extract_context(obs_dict)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±–æ—Ä –ø–æ–ª–∏—Ç–∏–∫–∏
            self._log_policy_selection(selected_policy, infos)
        
        return int(self.current_policy)
    
    def _flatten_observations(self, obs_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –ø–ª–æ—Å–∫–∏–π —Ç–µ–Ω–∑–æ—Ä"""
        obs_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —á–∞—Å—Ç–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        for key in ["self", "allies", "enemies"]:
            if key in obs_dict:
                tensor = obs_dict[key]
                if tensor.dim() > 2:  # –¥–ª—è –±–∞—Ç—á–µ–π
                    tensor = tensor.flatten(start_dim=1)
                elif tensor.dim() == 2:  # –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
                    tensor = tensor.flatten()
                obs_parts.append(tensor)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Å–∫–∏
        for key in ["allies_mask", "enemies_mask", "enemy_action_mask"]:
            if key in obs_dict:
                tensor = obs_dict[key].float()
                if tensor.dim() == 1:
                    obs_parts.append(tensor)
                else:
                    obs_parts.append(tensor.flatten(start_dim=1))
        
        if not obs_parts:
            return torch.zeros(64)  # fallback
        
        return torch.cat(obs_parts, dim=-1 if obs_parts[0].dim() > 1 else 0)
    
    def _prepare_policy_history(self) -> torch.Tensor:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫"""
        history_length = 10
        history_tensor = torch.zeros(1, self.num_policies * history_length)
        
        if len(self.policy_history) > 0:
            recent_history = list(self.policy_history)[-history_length:]
            for i, entry in enumerate(recent_history):
                policy_idx = entry['policy']
                history_tensor[0, i * self.num_policies + policy_idx] = 1.0
        
        return history_tensor
    
    def update_policy_performance(self, rewards: Dict[str, float], 
                                 infos: Optional[Dict] = None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏"""
        if not rewards:
            return
        
        # –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –ø–æ –≤—Å–µ–º –∞–≥–µ–Ω—Ç–∞–º
        avg_reward = np.mean(list(rewards.values()))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—É—â–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
        alpha = 0.1  # —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        policy_idx = int(self.current_policy)
        
        self.policy_performance[policy_idx] = (
            (1 - alpha) * self.policy_performance[policy_idx] + 
            alpha * avg_reward
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ infos
        if infos:
            for agent_id, info in infos.items():
                if isinstance(info, dict):
                    # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∞–≤–∏–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫–∏
                    if 'kills' in info:
                        self.policy_performance[policy_idx] += info['kills'] * 0.5
                    if 'hits_taken' in info:
                        self.policy_performance[policy_idx] -= info['hits_taken'] * 0.2
                    if 'missiles_fired' in info and 'hits_given' in info:
                        hit_rate = info.get('hits_given', 0) / max(1, info.get('missiles_fired', 1))
                        self.policy_performance[policy_idx] += hit_rate * 0.3
    
    def train_selector(self, batch_rewards: List[float], 
                      batch_contexts: List[torch.Tensor]):
        """–û–±—É—á–∞–µ—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ–ª–∏—Ç–∏–∫"""
        if len(batch_rewards) < 10:  # –Ω—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            return
        
        self.selector_optimizer.zero_grad()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32)
        contexts_tensor = torch.stack(batch_contexts)
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å
        policy_history_batch = torch.zeros(len(batch_rewards), self.num_policies * 10)
        performance_batch = torch.tile(
            torch.from_numpy(self.policy_performance), 
            (len(batch_rewards), 1)
        )
        
        policy_logits, values = self.selector_network(
            contexts_tensor, policy_history_batch, performance_batch
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º loss
        # Value loss
        value_loss = F.mse_loss(values.squeeze(), rewards_tensor)
        
        # Policy loss (REINFORCE)
        advantages = rewards_tensor - values.squeeze().detach()
        policy_probs = F.softmax(policy_logits, dim=-1)
        
        # –ü—Å–µ–≤–¥–æ-–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        pseudo_actions = torch.argmax(policy_logits, dim=-1)
        policy_loss = -torch.mean(
            torch.log(policy_probs.gather(1, pseudo_actions.unsqueeze(1))) * 
            advantages.unsqueeze(1)
        )
        
        # –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–π –±–æ–Ω—É—Å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        entropy_bonus = -0.01 * torch.mean(
            torch.sum(policy_probs * torch.log(policy_probs + 1e-8), dim=-1)
        )
        
        total_loss = value_loss + policy_loss + entropy_bonus
        
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(self.situation_analyzer.parameters()) + 
            list(self.selector_network.parameters()), 
            max_norm=0.5
        )
        self.selector_optimizer.step()
    
    def _log_policy_selection(self, selected_policy: int, infos: Optional[Dict]):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –≤—ã–±–æ—Ä –ø–æ–ª–∏—Ç–∏–∫–∏"""
        policy_name = PolicyType(selected_policy).name
        
        print(f"üéØ Policy selected: {policy_name} (step {len(self.policy_history)})")
        
        if self.last_situation_context:
            ctx = self.last_situation_context
            print(f"   Situation: {ctx.enemy_count} enemies, HP {ctx.own_hp:.2f}, "
                  f"Fuel {ctx.own_fuel:.2f}, Min enemy distance {ctx.enemy_distance_min:.2f}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫
        if len(self.policy_history) % 50 == 0:  # –∫–∞–∂–¥—ã–µ 50 –≤—ã–±–æ—Ä–æ–≤
            self._print_policy_statistics()
    
    def _print_policy_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫"""
        print(f"\nüìä Policy Usage Statistics:")
        total_usage = sum(self.policy_usage_count)
        
        for i in range(self.num_policies):
            policy_name = PolicyType(i).name
            usage_pct = (self.policy_usage_count[i] / max(1, total_usage)) * 100
            performance = self.policy_performance[i]
            
            print(f"   {policy_name:15}: {usage_pct:5.1f}% usage, "
                  f"Performance: {performance:+.3f}")
        
        print()
    
    def get_policy_explanation(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫–∏"""
        policy_name = self.current_policy.name
        
        explanations = {
            PolicyType.OBSERVER: "Maintaining safe distance, gathering intelligence on enemy positions and movements",
            PolicyType.AGGRESSIVE_SHOOTER: "Closing distance rapidly, prioritizing offensive engagement with available weapons",
            PolicyType.NORMAL_SHOOTER: "Balanced approach, engaging targets while maintaining tactical awareness",
            PolicyType.DEFENSIVE: "Prioritizing survival, using evasive maneuvers and defensive positioning",
            PolicyType.INTERCEPTOR: "Pursuing and engaging high-priority targets with speed and precision",
            PolicyType.SUPPORT: "Providing cover and assistance to allied aircraft in combat"
        }
        
        explanation = explanations.get(self.current_policy, "Unknown policy behavior")
        
        if self.last_situation_context:
            ctx = self.last_situation_context
            situation_desc = (
                f"Current situation: {ctx.enemy_count} enemies detected, "
                f"own status {ctx.own_hp:.0f}% HP, {ctx.own_fuel:.0f}% fuel, "
                f"{'radar lock detected' if ctx.radar_locked else 'no immediate threats'}"
            )
            return f"{policy_name}: {explanation}\n{situation_desc}"
        
        return f"{policy_name}: {explanation}"
    
    def save_state(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"""
        state = {
            'selector_network_state': self.selector_network.state_dict(),
            'situation_analyzer_state': self.situation_analyzer.state_dict(),
            'policy_performance': self.policy_performance.tolist(),
            'policy_usage_count': self.policy_usage_count.tolist(),
            'current_policy': int(self.current_policy),
            'policy_history': list(self.policy_history)
        }
        
        torch.save(state, filepath)
        print(f"üíæ Policy selector state saved: {filepath}")
    
    def load_state(self, filepath: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"""
        try:
            state = torch.load(filepath)
            
            self.selector_network.load_state_dict(state['selector_network_state'])
            self.situation_analyzer.load_state_dict(state['situation_analyzer_state'])
            self.policy_performance = np.array(state['policy_performance'])
            self.policy_usage_count = np.array(state['policy_usage_count'])
            self.current_policy = PolicyType(state['current_policy'])
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            self.policy_history.clear()
            for entry in state['policy_history']:
                self.policy_history.append(entry)
            
            print(f"üìÇ Policy selector state loaded: {filepath}")
            
        except Exception as e:
            print(f"‚ùå Failed to load policy selector state: {e}")

# –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
class SpecializedAircraftPolicy(PPOTorchPolicy):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫"""
    
    def __init__(self, *args, **kwargs):
        self.policy_type = kwargs.pop('policy_type', PolicyType.NORMAL_SHOOTER)
        super().__init__(*args, **kwargs)
        
        print(f"üõ©Ô∏è Initialized specialized policy: {self.policy_type.name}")
    
    def postprocess_trajectory(self, sample_batch: SampleBatch, 
                             other_agent_batches=None, episode=None):
        """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏"""
        
        # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        sample_batch = super().postprocess_trajectory(
            sample_batch, other_agent_batches, episode)
        
        # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
        if self.policy_type == PolicyType.OBSERVER:
            # –ù–∞–±–ª—é–¥–∞—Ç–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–≤–µ–¥–∫—É
            self._modify_rewards_for_observer(sample_batch)
        elif self.policy_type == PolicyType.AGGRESSIVE_SHOOTER:
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å—Ç—Ä–µ–ª–æ–∫ - –∑–∞ –∞—Ç–∞–∫–∏ –∏ —É—Ä–æ–Ω
            self._modify_rewards_for_aggressive(sample_batch)
        elif self.policy_type == PolicyType.DEFENSIVE:
            # –û–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω–∞—è - –∑–∞ —É–∫–ª–æ–Ω–µ–Ω–∏–µ –∏ –≤—ã–∂–∏–≤–∞–Ω–∏–µ
            self._modify_rewards_for_defensive(sample_batch)
        elif self.policy_type == PolicyType.SUPPORT:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ - –∑–∞ –ø–æ–º–æ—â—å —Å–æ—é–∑–Ω–∏–∫–∞–º
            self._modify_rewards_for_support(sample_batch)
        
        return sample_batch
    
    def _modify_rewards_for_observer(self, sample_batch: SampleBatch):
        """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è"""
        rewards = sample_batch["rewards"]
        infos = sample_batch.get("infos", [])
        
        for i, (reward, info) in enumerate(zip(rewards, infos)):
            if isinstance(info, dict):
                # –ë–æ–Ω—É—Å –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ
                if info.get("hp", 100) > 50:
                    rewards[i] += 0.1
                
                # –ë–æ–Ω—É—Å –∑–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—Ä–∞–≥–æ–≤ (–±–µ–∑ –∞—Ç–∞–∫–∏)
                if info.get("enemies_detected", 0) > 0 and info.get("missiles_fired", 0) == 0:
                    rewards[i] += 0.2
                
                # –®—Ç—Ä–∞—Ñ –∑–∞ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
                if info.get("missiles_fired", 0) > 0:
                    rewards[i] -= 0.1
    
    def _modify_rewards_for_aggressive(self, sample_batch: SampleBatch):
        """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏"""
        rewards = sample_batch["rewards"]
        infos = sample_batch.get("infos", [])
        
        for i, (reward, info) in enumerate(zip(rewards, infos)):
            if isinstance(info, dict):
                # –ë–æ–Ω—É—Å –∑–∞ –∞—Ç–∞–∫–∏ –∏ –ø–æ–ø–∞–¥–∞–Ω–∏—è
                missiles_fired = info.get("missiles_fired", 0)
                hits_given = info.get("hits_given", 0)
                
                rewards[i] += missiles_fired * 0.2  # –±–æ–Ω—É—Å –∑–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                rewards[i] += hits_given * 0.5      # –±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ –ø–æ–ø–∞–¥–∞–Ω–∏—è
                
                # –ë–æ–Ω—É—Å –∑–∞ —Å–±–ª–∏–∂–µ–Ω–∏–µ —Å –≤—Ä–∞–≥–∞–º–∏
                min_enemy_distance = info.get("min_enemy_distance", 1.0)
                if min_enemy_distance < 0.3:  # –±–ª–∏–∑–∫–æ –∫ –≤—Ä–∞–≥—É
                    rewards[i] += 0.3
    
    def _modify_rewards_for_defensive(self, sample_batch: SampleBatch):
        """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏"""
        rewards = sample_batch["rewards"]
        infos = sample_batch.get("infos", [])
        
        for i, (reward, info) in enumerate(zip(rewards, infos)):
            if isinstance(info, dict):
                # –ë–æ–Ω—É—Å –∑–∞ —É–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –∞—Ç–∞–∫
                hits_taken = info.get("hits_taken", 0)
                if hits_taken == 0:
                    rewards[i] += 0.2
                else:
                    rewards[i] -= hits_taken * 0.3
                
                # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ HP
                hp = info.get("hp", 100)
                rewards[i] += (hp / 100.0) * 0.1
    
    def _modify_rewards_for_support(self, sample_batch: SampleBatch):
        """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏"""
        rewards = sample_batch["rewards"]
        infos = sample_batch.get("infos", [])
        
        for i, (reward, info) in enumerate(zip(rewards, infos)):
            if isinstance(info, dict):
                # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–º–æ—â—å —Å–æ—é–∑–Ω–∏–∫–∞–º
                allies_supported = info.get("allies_supported", 0)
                rewards[i] += allies_supported * 0.3
                
                # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏–π
                team_coordination = info.get("team_coordination", 0.0)
                rewards[i] += team_coordination * 0.2

def create_specialized_policies(obs_space, act_space, model_config: Dict) -> Dict:
    """–°–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫"""
    
    policies = {}
    
    # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª–∏—Ç–∏–∫
    base_config = model_config.copy()
    
    for policy_type in PolicyType:
        if policy_type.value >= 6:  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 6 –ø–æ–ª–∏—Ç–∏–∫
            break
            
        # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ–¥ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        specialized_config = base_config.copy()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –ø–æ–ª–∏—Ç–∏–∫–∏
        if policy_type == PolicyType.OBSERVER:
            # –ù–∞–±–ª—é–¥–∞—Ç–µ–ª—å: –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –∞–Ω–∞–ª–∏–∑—É —Å–∏—Ç—É–∞—Ü–∏–∏
            specialized_config["custom_model_config"]["d_model"] = 192
            specialized_config["custom_model_config"]["nhead"] = 6
            specialized_config["custom_model_config"]["layers"] = 3
            
        elif policy_type == PolicyType.AGGRESSIVE_SHOOTER:
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π: –±—ã—Å—Ç—Ä—ã–µ —Ä–µ—à–µ–Ω–∏—è, –º–µ–Ω—å—à–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
            specialized_config["custom_model_config"]["d_model"] = 128
            specialized_config["custom_model_config"]["nhead"] = 4
            specialized_config["custom_model_config"]["layers"] = 2
            
        elif policy_type == PolicyType.DEFENSIVE:
            # –û–±–æ—Ä–æ–Ω–∏—Ç–µ–ª—å–Ω—ã–π: —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —É–≥—Ä–æ–∑
            specialized_config["custom_model_config"]["d_model"] = 160
            specialized_config["custom_model_config"]["nhead"] = 8
            specialized_config["custom_model_config"]["layers"] = 3
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
        policy_id = f"specialized_{policy_type.name.lower()}"
        policies[policy_id] = (
            SpecializedAircraftPolicy,
            obs_space,
            act_space,
            {
                "model": specialized_config,
                "policy_type": policy_type
            }
        )
        
        print(f"‚úàÔ∏è Created specialized policy: {policy_id}")
    
    return policies


class PolicySelectorWrapper:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫ —Å RLLib"""
    
    def __init__(self, obs_space, act_space, num_policies: int = 6):
        self.obs_space = obs_space
        self.act_space = act_space
        self.num_policies = num_policies
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä
        self.selector = PolicySelector(obs_space, num_policies)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
        base_model_config = {
            "custom_model": "aircraft_transformer",
            "custom_action_dist": "aircraft_actions",
            "custom_model_config": {
                "d_model": 256,
                "nhead": 8,
                "layers": 3,
                "max_aircraft": obs_space["allies"].shape[0],
                "max_enemies": obs_space["enemies"].shape[0],
            },
            "vf_share_layers": False,
        }
        
        self.specialized_policies = create_specialized_policies(
            obs_space, act_space, base_model_config
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_selections = 0
        self.selection_rewards = []
        self.current_policy_performance = {}
        
        print(f"üéØ Policy Selector Wrapper initialized")
        print(f"   Total specialized policies: {len(self.specialized_policies)}")
    
    def select_and_execute(self, algorithm, obs_dict: Dict, agent_id: str, 
                          infos: Optional[Dict] = None) -> Tuple[Any, int]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ"""
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(obs_dict, dict):
            obs_tensors = {}
            for key, value in obs_dict.items():
                if isinstance(value, np.ndarray):
                    obs_tensors[key] = torch.from_numpy(value).float()
                else:
                    obs_tensors[key] = value
        else:
            obs_tensors = obs_dict
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É
        selected_policy_idx = self.selector.select_policy(obs_tensors, infos)
        policy_type = PolicyType(selected_policy_idx)
        
        # –ü–æ–ª—É—á–∞–µ–º ID –ø–æ–ª–∏—Ç–∏–∫–∏
        policy_id = f"specialized_{policy_type.name.lower()}"
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É
        if policy_id in algorithm.policies:
            policy = algorithm.get_policy(policy_id)
            action, state, action_info = policy.compute_single_action(
                obs_dict, explore=True
            )
        else:
            # Fallback –Ω–∞ –æ—Å–Ω–æ–≤–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É
            policy = algorithm.get_policy("main")
            action, state, action_info = policy.compute_single_action(
                obs_dict, explore=True
            )
        
        self.total_selections += 1
        return action, selected_policy_idx
    
    def update_performance(self, rewards: Dict[str, float], 
                          infos: Optional[Dict] = None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫"""
        self.selector.update_policy_performance(rewards, infos)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
        if rewards:
            avg_reward = np.mean(list(rewards.values()))
            self.selection_rewards.append(avg_reward)
        
        # –û–±—É—á–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
        if len(self.selection_rewards) >= 100:
            contexts = []
            for _ in range(min(50, len(self.selection_rewards))):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Å–µ–≤–¥–æ-–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
                pseudo_context = torch.randn(32)  # —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                contexts.append(pseudo_context)
            
            recent_rewards = self.selection_rewards[-50:]
            self.selector.train_selector(recent_rewards, contexts)
            
            # –û—á–∏—â–∞–µ–º —á–∞—Å—Ç—å –∏—Å—Ç–æ—Ä–∏–∏
            self.selection_rewards = self.selection_rewards[-50:]
    
    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–∞"""
        return {
            "total_selections": self.total_selections,
            "policy_usage": self.selector.policy_usage_count.tolist(),
            "policy_performance": self.selector.policy_performance.tolist(),
            "current_policy": self.selector.current_policy.name,
            "avg_recent_reward": np.mean(self.selection_rewards[-20:]) if self.selection_rewards else 0.0
        }


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Ü–∏–∫–ª–æ–º –æ–±—É—á–µ–Ω–∏—è
class PolicySelectorCallback:
    """Callback –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫ –≤ –æ–±—É—á–µ–Ω–∏–µ"""
    
    def __init__(self, obs_space, act_space):
        self.obs_space = obs_space
        self.act_space = act_space
        self.selector_wrapper = None
        self.enabled = False
        
    def setup(self, algorithm, enable_selector: bool = True):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ callback'–∞"""
        self.enabled = enable_selector
        
        if self.enabled:
            self.selector_wrapper = PolicySelectorWrapper(
                self.obs_space, self.act_space
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –∞–ª–≥–æ—Ä–∏—Ç–º—É
            for policy_id, policy_spec in self.selector_wrapper.specialized_policies.items():
                if policy_id not in algorithm.policies:
                    algorithm.add_policy(
                        policy_id=policy_id,
                        policy_cls=policy_spec[0],
                        observation_space=policy_spec[1],
                        action_space=policy_spec[2],
                        config=policy_spec[3]
                    )
            
            print(f"üéØ Policy Selector enabled with {len(self.selector_wrapper.specialized_policies)} specialized policies")
    
    def on_episode_start(self, *, worker, base_env, policies, episode, **kwargs):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ —ç–ø–∏–∑–æ–¥–∞"""
        if self.enabled and self.selector_wrapper:
            # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —ç–ø–∏–∑–æ–¥–∞
            pass
    
    def on_episode_step(self, *, worker, base_env, episode, **kwargs):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —à–∞–≥–∞ —ç–ø–∏–∑–æ–¥–∞"""
        if not self.enabled or not self.selector_wrapper:
            return
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
        pass
    
    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞"""
        if not self.enabled or not self.selector_wrapper:
            return
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–∏–∑–æ–¥–∞
        if hasattr(episode, 'agent_rewards'):
            rewards = {aid: sum(episode.agent_rewards[aid]) for aid in episode.agent_rewards}
            self.selector_wrapper.update_performance(rewards)
    
    def on_train_result(self, *, algorithm, result, **kwargs):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.enabled or not self.selector_wrapper:
            return
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        selector_stats = self.selector_wrapper.get_statistics()
        
        custom_metrics = result.get("custom_metrics", {})
        custom_metrics.update({
            f"policy_selector_{k}": v for k, v in selector_stats.items()
            if isinstance(v, (int, float))
        })
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫
        if selector_stats["total_selections"] % 100 == 0:
            print(f"\nüéØ Policy Selector Statistics:")
            print(f"   Current policy: {selector_stats['current_policy']}")
            print(f"   Total selections: {selector_stats['total_selections']}")
            print(f"   Average recent reward: {selector_stats['avg_recent_reward']:.3f}")
            
            usage = np.array(selector_stats["policy_usage"])
            if usage.sum() > 0:
                usage_pct = (usage / usage.sum()) * 100
                for i, pct in enumerate(usage_pct):
                    if pct > 0:
                        policy_name = PolicyType(i).name
                        print(f"   {policy_name}: {pct:.1f}%")
        
        result["custom_metrics"] = custom_metrics


# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ª–∏—Ç–∏–∫
def analyze_policy_effectiveness(selector_wrapper: PolicySelectorWrapper, 
                               num_episodes: int = 100) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫"""
    
    print(f"üìä Analyzing policy effectiveness over {num_episodes} episodes...")
    
    analysis = {
        "policy_performance": selector_wrapper.selector.policy_performance.tolist(),
        "policy_usage": selector_wrapper.selector.policy_usage_count.tolist(),
        "total_selections": selector_wrapper.total_selections,
    }
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    performance = selector_wrapper.selector.policy_performance
    usage = selector_wrapper.selector.policy_usage_count
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    total_usage = usage.sum()
    usage_normalized = usage / max(total_usage, 1)
    
    # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    policy_scores = performance * usage_normalized  # –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    
    analysis["policy_rankings"] = []
    for i in np.argsort(policy_scores)[::-1]:  # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        if usage[i] > 0:  # —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
            policy_name = PolicyType(i).name
            analysis["policy_rankings"].append({
                "policy": policy_name,
                "performance": float(performance[i]),
                "usage_percent": float(usage_normalized[i] * 100),
                "weighted_score": float(policy_scores[i])
            })
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    analysis["recommendations"] = []
    
    # –ù–µ–¥–æ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    for i, score in enumerate(policy_scores):
        if performance[i] > np.mean(performance) and usage_normalized[i] < 0.1:
            policy_name = PolicyType(i).name
            analysis["recommendations"].append(
                f"Consider using {policy_name} more often - high performance but low usage"
            )
    
    # –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    for i, score in enumerate(policy_scores):
        if performance[i] < np.mean(performance) and usage_normalized[i] > 0.3:
            policy_name = PolicyType(i).name
            analysis["recommendations"].append(
                f"Reduce usage of {policy_name} - low performance but high usage"
            )
    
    print("‚úÖ Policy effectiveness analysis completed")
    return analysis

def create_policy_selector_config(base_config: Dict, enable_selector: bool = True) -> Dict:
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º –ø–æ–ª–∏—Ç–∏–∫"""
    
    if not enable_selector:
        return base_config
    
    # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = base_config.copy()
    
    # –î–æ–±–∞–≤–ª—è–µ–º callback –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫
    config["callbacks"] = PolicySelectorCallback
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏
    obs_space = config.get("observation_space")
    act_space = config.get("action_space") 
    
    if obs_space and act_space:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π wrapper –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫
        temp_wrapper = PolicySelectorWrapper(obs_space, act_space)
        specialized_policies = temp_wrapper.specialized_policies
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if "policies" in config:
            config["policies"].update(specialized_policies)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å–ø–∏—Å–∫—É –æ–±—É—á–∞–µ–º—ã—Ö –ø–æ–ª–∏—Ç–∏–∫
        if "policies_to_train" in config:
            specialized_ids = list(specialized_policies.keys())
            config["policies_to_train"].extend(specialized_ids)
    
    print(f"üéØ Policy selector configuration created")
    print(f"   Selector enabled: {enable_selector}")
    
    return config


if __name__ == "__main__":
    print("üéØ Policy Selector System for Air-to-Air Combat")
    print("\nFeatures:")
    print("- Adaptive policy selection based on situation analysis")
    print("- 6 specialized policies: Observer, Aggressive Shooter, Normal Shooter, Defensive, Interceptor, Support")
    print("- Neural network-based situation analyzer")
    print("- Performance tracking and learning")
    print("- Selection every 10 steps with exploration")
    print("- Integration with RLLib training pipeline")
    print("\nSpecialized Policies:")
    
    for policy_type in PolicyType:
        if policy_type.value >= 6:
            break
        print(f"  {policy_type.value + 1}. {policy_type.name}: ", end="")
        
        descriptions = {
            PolicyType.OBSERVER: "Passive reconnaissance, intelligence gathering",
            PolicyType.AGGRESSIVE_SHOOTER: "Active engagement, close-range combat",
            PolicyType.NORMAL_SHOOTER: "Balanced approach, general combat",
            PolicyType.DEFENSIVE: "Survival focus, evasive maneuvers",
            PolicyType.INTERCEPTOR: "Target pursuit, high-speed engagement",
            PolicyType.SUPPORT: "Team coordination, ally assistance"
        }
        
        print(descriptions.get(policy_type, "Unknown specialization"))